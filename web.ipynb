{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (10.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping webpage: https://www.wikipedia.org/\n",
      "Page title: Wikipedia\n",
      "Found 2 image(s).\n",
      "Retrieving image: https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png\n",
      "Image saved as 'images/retrieved_image.png'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# Step 1: Scrape a webpage\n",
    "def scrape_webpage(url):\n",
    "    print(f\"Scraping webpage: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(f\"Page title: {soup.title.string}\")\n",
    "    \n",
    "    # Retrieve and print all image URLs\n",
    "    image_tags = soup.find_all('img')\n",
    "    image_urls = [img['src'] for img in image_tags if 'src' in img.attrs]\n",
    "    print(f\"Found {len(image_urls)} image(s).\")\n",
    "    \n",
    "    return image_urls\n",
    "\n",
    "# Step 2: Retrieve an image over HTTP\n",
    "def retrieve_image(image_url):\n",
    "    print(f\"Retrieving image: {image_url}\")\n",
    "    response = requests.get(image_url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Open the image with PIL\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img.show()  # Display the image\n",
    "    \n",
    "    # Save the image locally\n",
    "    if not os.path.exists('images'):\n",
    "        os.makedirs('images')\n",
    "    img.save(f\"images/retrieved_image.{img.format.lower()}\")\n",
    "    print(f\"Image saved as 'images/retrieved_image.{img.format.lower()}'.\")\n",
    "    \n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Example URL to scrape (you can use any public webpage)\n",
    "        webpage_url = \"https://www.wikipedia.org/\"\n",
    "        \n",
    "        # Scrape the webpage\n",
    "        image_urls = scrape_webpage(webpage_url)\n",
    "        \n",
    "        # Retrieve the first image if available\n",
    "        if image_urls:\n",
    "            first_image_url = image_urls[0]\n",
    "            # Handle relative URLs\n",
    "            if not first_image_url.startswith(\"http\"):\n",
    "                first_image_url = webpage_url + first_image_url\n",
    "            retrieve_image(first_image_url)\n",
    "        else:\n",
    "            print(\"No images found on the webpage.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Text: <html><head><title>403 Forbidden</title><link href='//fonts.bunny.net/css?family=Rubik:300,400,500' rel='stylesheet' type='text/css'><style>html, body { width: 100%; margin: 0; padding: 0; text-align: center; font-family: 'Rubik'; background-image: url('data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iMjg4MHB4IiBoZWlnaHQ9IjE0MjRweCIgdmlld0JveD0iMCAwIDI4ODAgMTQyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIj4KICAgIDxkZWZzPgogICAgICAgIDxyYWRpYWxHcmFkaWVudCBjeD0iNDguNDU0MDQyMiUiIGN5PSIyNy4wMTE5NjQ1JSIgZng9IjQ4LjQ1NDA0MjIlIiBmeT0iMjcuMDExOTY0NSUiIHI9IjcwLjg3MDg1MTQlIiBncmFkaWVudFRyYW5zZm9ybT0idHJhbnNsYXRlKDAuNDg0NTQwLDAuMjcwMTIwKSxzY2FsZSgwLjQ5NDQ0NCwxKSxyb3RhdGUoOTApLHRyYW5zbGF0ZSgtMC40ODQ1NDAsLTAuMjcwMTIwKSIgaWQ9InJhZGlhbEdyYWRpZW50LTEiPgogICAgICAgICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAyNjQ5IiBvZmZzZXQ9IjAlIj48L3N0b3A+CiAgICAgICAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwNTFGMzciIG9mZnNldD0iMTAwJSI+PC9zdG9wPgogICAgICAgIDwvcmFkaWFsR3JhZGllbnQ+CiAgICAgICAgPHJlY3QgaWQ9InBhdGgtMiIgeD0iMCIgeT0iMCIgd2lkdGg9IjI4ODAiIGhlaWdodD0iMTQyNCI+PC9yZWN0PgogICAgPC9kZWZzPgogICAgPGcgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCI+CiAgICAgICAgPGcgaWQ9IkhvbWVwYWdlLUNvcHkiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDAsIC01Mjk1KSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0xNyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMCwgNTI5NSkiPgogICAgICAgICAgICAgICAgPGcgaWQ9Ikdyb3VwLTM0Ij4KICAgICAgICAgICAgICAgICAgICA8bWFzayBpZD0ibWFzay0zIiBmaWxsPSJ3aGl0ZSI+CiAgICAgICAgICAgICAgICAgICAgICAgIDx1c2UgeGxpbms6aHJlZj0iI3BhdGgtMiI+PC91c2U+CiAgICAgICAgICAgICAgICAgICAgPC9tYXNrPgogICAgICAgICAgICAgICAgICAgIDx1c2UgaWQ9Ik1hc2siIGZpbGw9InVybCgjcmFkaWFsR3JhZGllbnQtMSkiIHhsaW5rOmhyZWY9IiNwYXRoLTIiPjwvdXNlPgogICAgICAgICAgICAgICAgPC9nPgogICAgICAgICAgICA8L2c+CiAgICAgICAgPC9nPgogICAgPC9nPgo8L3N2Zz4='); background-repeat: no-repeat; background-position: bottom center; background-size: cover; color: white; height: 100%; background-color: #051f37; } h1 {margin-bottom: 0px;font-weight: bold;font-size: 140px;font-weight: 500;padding-top: 130px;margin-bottom: -35px;}h2 {font-size: 45px;color: white; font-weight: 200;}</style></head><body><div id='content'><h1 style='margin-bottom: -35px;'>403</h1><h2>Forbidden</h2></div></body></html>\n",
      "Response Headers: {'Date': 'Tue, 10 Dec 2024 09:50:05 GMT', 'Content-Type': 'text/html', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding', 'Server': 'BunnyCDN-SG1-747', 'CDN-PullZone': '93447', 'CDN-Uid': '8fa3a04a-75d9-4707-8056-b7b33c8ac7fe', 'CDN-RequestCountryCode': 'IN', 'CDN-RequestId': '2026e1289a799af5ced3d0ca3164dbc3', 'Content-Encoding': 'gzip'}\n",
      "Failed to fetch data. HTTP Status Code: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def fetch_weather_data(city):\n",
    "    \"\"\"\n",
    "    Fetch weather data from a web service returning XML responses.\n",
    "    \"\"\"\n",
    "    # Example URL for a free weather API with XML response (replace with a valid endpoint)\n",
    "    url = f\"http://api.weatherapi.com/v1/current.xml?key=YOUR_API_KEY&q={city}\"\n",
    "    \n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url)\n",
    "    print(\"Response Text:\", response.text)\n",
    "    print(\"Response Headers:\", response.headers)\n",
    "\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Successfully fetched the XML data!\")\n",
    "        return response.content\n",
    "    else:\n",
    "        print(f\"Failed to fetch data. HTTP Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def parse_xml_data(xml_data):\n",
    "    \"\"\"\n",
    "    Parse the XML data and extract specific information.\n",
    "    \"\"\"\n",
    "    # Parse the XML content\n",
    "    root = ET.fromstring(xml_data)\n",
    "    \n",
    "    # Extract specific data (customize based on the XML structure)\n",
    "    location = root.find(\"location/name\").text\n",
    "    region = root.find(\"location/region\").text\n",
    "    country = root.find(\"location/country\").text\n",
    "    temperature = root.find(\"current/temp_c\").text\n",
    "    condition = root.find(\"current/condition/text\").text\n",
    "    \n",
    "    # Print the extracted information\n",
    "    print(f\"Location: {location}, {region}, {country}\")\n",
    "    print(f\"Temperature: {temperature}Â°C\")\n",
    "    print(f\"Condition: {condition}\")\n",
    "\n",
    "def main():\n",
    "    # City for which we want the weather data\n",
    "    city = \"London\"\n",
    "    \n",
    "    # Fetch and parse the XML data\n",
    "    xml_data = fetch_weather_data(city)\n",
    "    if xml_data:\n",
    "        parse_xml_data(xml_data)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
